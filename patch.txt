diff --git a/ldm/models/diffusion/ddim.py b/ldm/models/diffusion/ddim.py
index c6cfd57..2b7db08 100644
--- a/ldm/models/diffusion/ddim.py
+++ b/ldm/models/diffusion/ddim.py
@@ -9,7 +9,9 @@ from ldm.modules.diffusionmodules.util import make_ddim_sampling_parameters, mak
 
 class DDIMSampler(object):
     def __init__(self, model, schedule="linear", device=torch.device("cuda"), **kwargs):
+        print("DDIMSampler being created here")
         super().__init__()
+        print("DDIMSampler being created here")
         self.model = model
         self.ddpm_num_timesteps = model.num_timesteps
         self.schedule = schedule
@@ -76,6 +78,8 @@ class DDIMSampler(object):
                unconditional_conditioning=None, # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...
                dynamic_threshold=None,
                ucg_schedule=None,
+               target_index=-1,
+               original_batch_size=None,
                **kwargs
                ):
         if conditioning is not None:
@@ -95,6 +99,7 @@ class DDIMSampler(object):
                 if conditioning.shape[0] != batch_size:
                     print(f"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}")
 
+        print("Sampling here")
         self.make_schedule(ddim_num_steps=S, ddim_eta=eta, verbose=verbose)
         # sampling
         C, H, W = shape
@@ -116,7 +121,9 @@ class DDIMSampler(object):
                                                     unconditional_guidance_scale=unconditional_guidance_scale,
                                                     unconditional_conditioning=unconditional_conditioning,
                                                     dynamic_threshold=dynamic_threshold,
-                                                    ucg_schedule=ucg_schedule
+                                                    ucg_schedule=ucg_schedule,
+                                                    target_index=target_index,
+                                                    original_batch_size=original_batch_size,
                                                     )
         return samples, intermediates
 
@@ -127,11 +134,32 @@ class DDIMSampler(object):
                       mask=None, x0=None, img_callback=None, log_every_t=100,
                       temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,
                       unconditional_guidance_scale=1., unconditional_conditioning=None, dynamic_threshold=None,
-                      ucg_schedule=None):
+                      ucg_schedule=None, target_index=-1, original_batch_size=None):
+        print("DDIM sampling")
         device = self.model.betas.device
         b = shape[0]
         if x_T is None:
-            img = torch.randn(shape, device=device)
+            # Generate noise for the entire batch as before if target_index is not specified
+            print("target shape {}".format(shape))
+            if target_index == -1:
+                print("target index not set")
+                img = torch.randn(shape, device=device)
+            # Generate noise for a specific image in the batch if batch size is 1
+            elif shape[0] == 1:
+                print("reproducing target image{}".format(shape))
+                if target_index >= original_batch_size:
+                    raise ValueError("target_index must be less than the original batch size")
+                # Generate noise for the entire batch and return only the noise for the specified index
+                size_list = list(shape) 
+                size_list[0] = original_batch_size 
+                new_shape = torch.Size(size_list)
+                print("original shape {}".format(new_shape))
+                img_tmp = torch.randn(new_shape, device=device)
+                # Repeat noise for the entire batch but get only one image
+                img = img_tmp[target_index, :].unsqueeze(0)
+            # Raise an error if target_index is specified and batch size is greater than 1
+            else:
+                raise ValueError("You cannot have target_index set when batch_size > 1")
         else:
             img = x_T
 
@@ -167,7 +195,7 @@ class DDIMSampler(object):
                                       corrector_kwargs=corrector_kwargs,
                                       unconditional_guidance_scale=unconditional_guidance_scale,
                                       unconditional_conditioning=unconditional_conditioning,
-                                      dynamic_threshold=dynamic_threshold)
+                                      dynamic_threshold=dynamic_threshold, target_index=target_index,original_batch_size=original_batch_size)
             img, pred_x0 = outs
             if callback: callback(i)
             if img_callback: img_callback(pred_x0, i)
@@ -182,7 +210,7 @@ class DDIMSampler(object):
     def p_sample_ddim(self, x, c, t, index, repeat_noise=False, use_original_steps=False, quantize_denoised=False,
                       temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,
                       unconditional_guidance_scale=1., unconditional_conditioning=None,
-                      dynamic_threshold=None):
+                      dynamic_threshold=None,target_index=-1,original_batch_size=None):
         b, *_, device = *x.shape, x.device
 
         if unconditional_conditioning is None or unconditional_guidance_scale == 1.:
@@ -245,7 +273,7 @@ class DDIMSampler(object):
 
         # direction pointing to x_t
         dir_xt = (1. - a_prev - sigma_t**2).sqrt() * e_t
-        noise = sigma_t * noise_like(x.shape, device, repeat_noise) * temperature
+        noise = sigma_t * noise_like(x.shape, device, repeat_noise, target_index=target_index, original_batch_size=original_batch_size) * temperature
         if noise_dropout > 0.:
             noise = torch.nn.functional.dropout(noise, p=noise_dropout)
         x_prev = a_prev.sqrt() * pred_x0 + dir_xt + noise
diff --git a/ldm/modules/diffusionmodules/util.py b/ldm/modules/diffusionmodules/util.py
index 637363d..6175124 100644
--- a/ldm/modules/diffusionmodules/util.py
+++ b/ldm/modules/diffusionmodules/util.py
@@ -264,7 +264,47 @@ class HybridConditioner(nn.Module):
         return {'c_concat': [c_concat], 'c_crossattn': [c_crossattn]}
 
 
-def noise_like(shape, device, repeat=False):
-    repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))
-    noise = lambda: torch.randn(shape, device=device)
-    return repeat_noise() if repeat else noise()
\ No newline at end of file
+def noise_like(shape, device, repeat=False, target_index=-1, original_batch_size=None):
+    """
+    Generates noise of a given shape with a specified device.
+
+    Args:
+        shape (tuple): The shape of the noise tensor.
+        device (torch.device): The device on which to generate the noise tensor.
+        repeat (bool): Whether to repeat the same noise across the batch dimension.
+        target_index (int): The index of the image in the batch for which to generate noise. Default: -1 (generate noise for the entire batch).
+        original_batch_size (int): The original batch size before sampling from DDIM. This argument is only used if target_index is specified and batch size is 1. Default: None.
+
+    Returns:
+        torch.Tensor: The generated noise tensor.
+
+    Raises:
+        ValueError: If target_index is specified and batch size is greater than 1.
+        ValueError: If target_index is greater than or equal to the batch size.
+    """
+
+    # Generate noise for the entire batch as before if target_index is not specified
+    if target_index == -1:
+        repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(shape[0], *((1,) * (len(shape) - 1)))
+        noise = lambda: torch.randn(shape, device=device)
+        return repeat_noise() if repeat else noise()
+
+    # Generate noise for a specific image in the batch if batch size is 1
+    elif shape[0] == 1:
+        # Repeat noise for the entire batch but generate separate noise for each image
+        repeat_noise = lambda: torch.randn((1, *shape[1:]), device=device).repeat(original_batch_size, *((1,) * (len(shape) - 1)))
+
+        # Generate noise for the entire batch and return only the noise for the specified index
+        size_list = list(shape)  # convert to list
+        size_list[0] = original_batch_size  # change the first dimension to 6
+        new_shape = torch.Size(size_list)  # create a new torch.Size object with the updated list
+        noise = lambda: torch.randn(new_shape, device=device)
+        if target_index >= original_batch_size:
+            raise ValueError("target_index must be less than the original batch size")
+        return noise()[target_index, :].unsqueeze(0)
+
+    # Raise an error if target_index is specified and batch size is greater than 1
+    else:
+        raise ValueError("You cannot have target_index set when batch_size > 1")
+    
+    
\ No newline at end of file
diff --git a/scripts/gradio/depth2img.py b/scripts/gradio/depth2img.py
index c791a4d..229e951 100644
--- a/scripts/gradio/depth2img.py
+++ b/scripts/gradio/depth2img.py
@@ -17,8 +17,10 @@ torch.set_grad_enabled(False)
 
 
 def initialize_model(config, ckpt):
+    print(ckpt)
     config = OmegaConf.load(config)
     model = instantiate_from_config(config.model)
+    print(ckpt)
     model.load_state_dict(torch.load(ckpt)["state_dict"], strict=False)
 
     device = torch.device(
@@ -77,7 +79,7 @@ def paint(sampler, image, prompt, t_enc, seed, scale, num_samples=1, callback=No
             cc = model.depth_model(cc)
             depth_min, depth_max = torch.amin(cc, dim=[1, 2, 3], keepdim=True), torch.amax(cc, dim=[1, 2, 3],
                                                                                            keepdim=True)
-            display_depth = (cc - depth_min) / (depth_max - depth_min)
+            display_depth = 1 - (cc - depth_min) / (depth_max - depth_min)
             depth_image = Image.fromarray(
                 (display_depth[0, 0, ...].cpu().numpy() * 255.).astype(np.uint8))
             cc = torch.nn.functional.interpolate(
diff --git a/scripts/img2img.py b/scripts/img2img.py
index 9085ba9..3bc3d9f 100644
--- a/scripts/img2img.py
+++ b/scripts/img2img.py
@@ -1,6 +1,8 @@
 """make variations of input image"""
 
 import argparse, os
+import urllib.parse
+import urllib.request
 import PIL
 import torch
 import numpy as np
@@ -47,7 +49,36 @@ def load_model_from_config(config, ckpt, verbose=False):
 
 
 def load_img(path):
+    if urllib.parse.urlparse(path).scheme:
+        # If `path` is a URL
+        try:
+            # If URL is found
+            filename = os.path.basename(path)
+
+            opener = urllib.request.build_opener()
+            opener.addheaders = [
+                (
+                    "User-Agent",
+                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36",
+                )
+            ]
+
+            urllib.request.install_opener(opener)
+            urllib.request.urlretrieve(path, filename)
+            path = filename
+        except:
+            filename = os.path.basename(
+                "https://chatgenius.1motion.ch/static/media/picture_test.png"
+            )
+            urllib.request.urlretrieve(path, filename)
+            path = filename
+    assert os.path.isfile(path)
     image = Image.open(path).convert("RGB")
+    width, height = image.size
+    new_height = 768
+    new_width = int((width / height) * new_height)
+    print(f"Initial size ({width}, {height}) from {path}")
+    image = image.resize((new_width, new_height), Image.ANTIALIAS)
     w, h = image.size
     print(f"loaded input image of size ({w}, {h}) from {path}")
     w, h = map(lambda x: x - x % 64, (w, h))  # resize to integer multiple of 64
@@ -55,7 +86,7 @@ def load_img(path):
     image = np.array(image).astype(np.float32) / 255.0
     image = image[None].transpose(0, 3, 1, 2)
     image = torch.from_numpy(image)
-    return 2. * image - 1.
+    return 2.0 * image - 1.0
 
 
 def main():
@@ -66,14 +97,19 @@ def main():
         type=str,
         nargs="?",
         default="a painting of a virus monster playing guitar",
-        help="the prompt to render"
+        help="the prompt to render",
     )
 
     parser.add_argument(
-        "--init-img",
+        "--negative_prompt",
         type=str,
         nargs="?",
-        help="path to the input image"
+        default="ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, out of frame, ugly, extra limbs, bad anatomy, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, mutated hands, fused fingers, too many fingers, long neck",
+        help="the prompt not to render",
+    )
+
+    parser.add_argument(
+        "--init-img", type=str, nargs="?", help="path or url to the input image"
     )
 
     parser.add_argument(
@@ -81,7 +117,7 @@ def main():
         type=str,
         nargs="?",
         help="dir to write results to",
-        default="outputs/img2img-samples"
+        default="outputs/img2img-samples",
     )
 
     parser.add_argument(
@@ -93,7 +129,7 @@ def main():
 
     parser.add_argument(
         "--fixed_code",
-        action='store_true',
+        action="store_true",
         help="if enabled, uses the same starting code across all samples ",
     )
 
@@ -178,27 +214,33 @@ def main():
         type=str,
         help="evaluate at this precision",
         choices=["full", "autocast"],
-        default="autocast"
+        default="autocast",
     )
 
     opt = parser.parse_args()
     seed_everything(opt.seed)
 
     config = OmegaConf.load(f"{opt.config}")
+    print("config loaded")
     model = load_model_from_config(config, f"{opt.ckpt}")
+    print("model loaded in ram")
 
     device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
+    print("model transferring to cuda")
     model = model.to(device)
+    print("model transferred to cuda")
 
     sampler = DDIMSampler(model)
 
     os.makedirs(opt.outdir, exist_ok=True)
     outpath = opt.outdir
 
-    print("Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...")
+    print(
+        "Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)..."
+    )
     wm = "SDV2"
     wm_encoder = WatermarkEncoder()
-    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))
+    wm_encoder.set_watermark("bytes", wm.encode("utf-8"))
 
     batch_size = opt.n_samples
     n_rows = opt.n_rows if opt.n_rows > 0 else batch_size
@@ -213,19 +255,24 @@ def main():
             data = f.read().splitlines()
             data = list(chunk(data, batch_size))
 
+    negative_prompt = opt.negative_prompt
+    print("Negative prompts: {}".format(negative_prompt))
     sample_path = os.path.join(outpath, "samples")
     os.makedirs(sample_path, exist_ok=True)
     base_count = len(os.listdir(sample_path))
     grid_count = len(os.listdir(outpath)) - 1
 
-    assert os.path.isfile(opt.init_img)
     init_image = load_img(opt.init_img).to(device)
-    init_image = repeat(init_image, '1 ... -> b ...', b=batch_size)
-    init_latent = model.get_first_stage_encoding(model.encode_first_stage(init_image))  # move to latent space
+    init_image = repeat(init_image, "1 ... -> b ...", b=batch_size)
+    init_latent = model.get_first_stage_encoding(
+        model.encode_first_stage(init_image)
+    )  # move to latent space
 
-    sampler.make_schedule(ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False)
+    sampler.make_schedule(
+        ddim_num_steps=opt.ddim_steps, ddim_eta=opt.ddim_eta, verbose=False
+    )
 
-    assert 0. <= opt.strength <= 1., 'can only work with strength in [0.0, 1.0]'
+    assert 0.0 <= opt.strength <= 1.0, "can only work with strength in [0.0, 1.0]"
     t_enc = int(opt.strength * opt.ddim_steps)
     print(f"target t_enc is {t_enc} steps")
 
@@ -238,22 +285,35 @@ def main():
                     for prompts in tqdm(data, desc="data"):
                         uc = None
                         if opt.scale != 1.0:
-                            uc = model.get_learned_conditioning(batch_size * [""])
+                            uc = model.get_learned_conditioning(
+                                batch_size * [negative_prompt]
+                            )
                         if isinstance(prompts, tuple):
                             prompts = list(prompts)
                         c = model.get_learned_conditioning(prompts)
 
                         # encode (scaled latent)
-                        z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc] * batch_size).to(device))
+                        z_enc = sampler.stochastic_encode(
+                            init_latent, torch.tensor([t_enc] * batch_size).to(device)
+                        )
                         # decode it
-                        samples = sampler.decode(z_enc, c, t_enc, unconditional_guidance_scale=opt.scale,
-                                                 unconditional_conditioning=uc, )
+                        samples = sampler.decode(
+                            z_enc,
+                            c,
+                            t_enc,
+                            unconditional_guidance_scale=opt.scale,
+                            unconditional_conditioning=uc,
+                        )
 
                         x_samples = model.decode_first_stage(samples)
-                        x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)
+                        x_samples = torch.clamp(
+                            (x_samples + 1.0) / 2.0, min=0.0, max=1.0
+                        )
 
                         for x_sample in x_samples:
-                            x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')
+                            x_sample = 255.0 * rearrange(
+                                x_sample.cpu().numpy(), "c h w -> h w c"
+                            )
                             img = Image.fromarray(x_sample.astype(np.uint8))
                             img = put_watermark(img, wm_encoder)
                             img.save(os.path.join(sample_path, f"{base_count:05}.png"))
@@ -262,14 +322,14 @@ def main():
 
                 # additionally, save as grid
                 grid = torch.stack(all_samples, 0)
-                grid = rearrange(grid, 'n b c h w -> (n b) c h w')
+                grid = rearrange(grid, "n b c h w -> (n b) c h w")
                 grid = make_grid(grid, nrow=n_rows)
 
                 # to image
-                grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()
+                grid = 255.0 * rearrange(grid, "c h w -> h w c").cpu().numpy()
                 grid = Image.fromarray(grid.astype(np.uint8))
                 grid = put_watermark(grid, wm_encoder)
-                grid.save(os.path.join(outpath, f'grid-{grid_count:04}.png'))
+                grid.save(os.path.join(outpath, f"grid-{grid_count:04}.png"))
                 grid_count += 1
 
     print(f"Your samples are ready and waiting for you here: \n{outpath} \nEnjoy.")
diff --git a/scripts/txt2img.py b/scripts/txt2img.py
index 9d955e3..490143c 100644
--- a/scripts/txt2img.py
+++ b/scripts/txt2img.py
@@ -1,3 +1,4 @@
+import io
 import argparse, os
 import cv2
 import torch
@@ -13,11 +14,18 @@ from torch import autocast
 from contextlib import nullcontext
 from imwatermark import WatermarkEncoder
 
+import sys
+sys.path.insert(0, '/cache/flo_398494/stablediffusion/')
 from ldm.util import instantiate_from_config
 from ldm.models.diffusion.ddim import DDIMSampler
 from ldm.models.diffusion.plms import PLMSSampler
 from ldm.models.diffusion.dpm_solver import DPMSolverSampler
 
+import inspect
+
+sampler_source_file = inspect.getsourcefile(DDIMSampler)
+print(sampler_source_file)
+
 torch.set_grad_enabled(False)
 
 def chunk(it, size):
@@ -60,6 +68,14 @@ def parse_args():
         default="a professional photograph of an astronaut riding a triceratops",
         help="the prompt to render"
     )
+    parser.add_argument(
+        "--negative_prompt",
+        type=str,
+        nargs="?",
+        default="ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, out of frame, ugly, extra limbs, bad anatomy, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, mutated hands, fused fingers, too many fingers, long neck",
+        help="the prompt not to render"
+    )
+    
     parser.add_argument(
         "--outdir",
         type=str,
@@ -139,7 +155,7 @@ def parse_args():
     parser.add_argument(
         "--scale",
         type=float,
-        default=9.0,
+        default=7.0,
         help="unconditional guidance scale: eps = eps(x, empty) + scale * (eps(x, cond) - eps(x, empty))",
     )
     parser.add_argument(
@@ -199,6 +215,22 @@ def parse_args():
         action='store_true',
         help="Use bfloat16",
     )
+    parser.add_argument(
+        '--original_batch_size',
+        type=int,
+        default=None,
+        help='Original batch size before sampling from DDIM'
+    )
+    parser.add_argument(
+        '--target_index',
+        type=int,
+        default=-1,
+        help='Index of the image in the batch for which to generate noise. Default: -1 (generate noise for the entire batch)'
+    )
+
+
+
+    
     opt = parser.parse_args()
     return opt
 
@@ -219,10 +251,13 @@ def main(opt):
     model = load_model_from_config(config, f"{opt.ckpt}", device)
 
     if opt.plms:
+        print("PLMSSampler")
         sampler = PLMSSampler(model, device=device)
     elif opt.dpm:
+        print("DPMSolverSampler")
         sampler = DPMSolverSampler(model, device=device)
     else:
+        print("DDIMSampler2")
         sampler = DDIMSampler(model, device=device)
 
     os.makedirs(opt.outdir, exist_ok=True)
@@ -247,6 +282,8 @@ def main(opt):
             data = [p for p in data for i in range(opt.repeat)]
             data = list(chunk(data, batch_size))
 
+    negative_prompt = opt.negative_prompt
+    print("Negative prompts: {}".format(negative_prompt))
     sample_path = os.path.join(outpath, "samples")
     os.makedirs(sample_path, exist_ok=True)
     sample_count = 0
@@ -310,7 +347,10 @@ def main(opt):
         print("Running a forward pass to initialize optimizations")
         uc = None
         if opt.scale != 1.0:
-            uc = model.get_learned_conditioning(batch_size * [""])
+            print("Scale at 1.0")
+            uc = model.get_learned_conditioning(batch_size * [negative_prompt])
+        else:
+            print("All fine")
         if isinstance(prompts, tuple):
             prompts = list(prompts)
 
@@ -339,7 +379,7 @@ def main(opt):
                 for prompts in tqdm(data, desc="data"):
                     uc = None
                     if opt.scale != 1.0:
-                        uc = model.get_learned_conditioning(batch_size * [""])
+                        uc = model.get_learned_conditioning(batch_size * [negative_prompt])
                     if isinstance(prompts, tuple):
                         prompts = list(prompts)
                     c = model.get_learned_conditioning(prompts)
@@ -352,7 +392,9 @@ def main(opt):
                                                      unconditional_guidance_scale=opt.scale,
                                                      unconditional_conditioning=uc,
                                                      eta=opt.ddim_eta,
-                                                     x_T=start_code)
+                                                     x_T=start_code,
+                                                     target_index=opt.target_index,
+                                                     original_batch_size=opt.original_batch_size)
 
                     x_samples = model.decode_first_stage(samples)
                     x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)
@@ -382,6 +424,184 @@ def main(opt):
     print(f"Your samples are ready and waiting for you here: \n{outpath} \n"
           f" \nEnjoy.")
 
+model = None
+device = None
+config = None
+
+def txt2img(opt):
+    global model
+    global device
+    global config
+    seed_everything(opt.seed)
+    if model is None:
+        config = OmegaConf.load(f"{opt.config}")
+        device = torch.device("cuda") if opt.device == "cuda" else torch.device("cpu")
+        model = load_model_from_config(config, f"{opt.ckpt}", device)
+
+
+    if opt.plms:
+        print("PLMSSampler")
+        sampler = PLMSSampler(model, device=device)
+    elif opt.dpm:
+        print("DPMSolverSampler")
+        sampler = DPMSolverSampler(model, device=device)
+    else:
+        print("DDIMSampler2")
+        sampler = DDIMSampler(model, device=device)
+
+    #os.makedirs(opt.outdir, exist_ok=True)
+    #outpath = opt.outdir
+
+    print("Creating invisible watermark encoder (see https://github.com/ShieldMnt/invisible-watermark)...")
+    wm = "SDV2"
+    wm_encoder = WatermarkEncoder()
+    wm_encoder.set_watermark('bytes', wm.encode('utf-8'))
+
+    batch_size = opt.n_samples
+    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size
+    if not opt.from_file:
+        prompt = opt.prompt
+        assert prompt is not None
+        data = [batch_size * [prompt]]
+
+    else:
+        print(f"reading prompts from {opt.from_file}")
+        with open(opt.from_file, "r") as f:
+            data = f.read().splitlines()
+            data = [p for p in data for i in range(opt.repeat)]
+            data = list(chunk(data, batch_size))
+
+    negative_prompt = opt.negative_prompt
+    print("Negative prompts: {}".format(negative_prompt))
+    #sample_path = os.path.join(outpath, "samples")
+    #os.makedirs(sample_path, exist_ok=True)
+    sample_count = 0
+    #base_count = len(os.listdir(sample_path))
+    #grid_count = len(os.listdir(outpath)) - 1
+
+    start_code = None
+    if opt.fixed_code:
+        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)
+
+    if opt.torchscript or opt.ipex:
+        transformer = model.cond_stage_model.model
+        unet = model.model.diffusion_model
+        decoder = model.first_stage_model.decoder
+        additional_context = torch.cpu.amp.autocast() if opt.bf16 else nullcontext()
+        shape = [opt.C, opt.H // opt.f, opt.W // opt.f]
+
+        if opt.bf16 and not opt.torchscript and not opt.ipex:
+            raise ValueError('Bfloat16 is supported only for torchscript+ipex')
+        if opt.bf16 and unet.dtype != torch.bfloat16:
+            raise ValueError("Use configs/stable-diffusion/intel/ configs with bf16 enabled if " +
+                             "you'd like to use bfloat16 with CPU.")
+        if unet.dtype == torch.float16 and device == torch.device("cpu"):
+            raise ValueError("Use configs/stable-diffusion/intel/ configs for your model if you'd like to run it on CPU.")
+
+        if opt.ipex:
+            import intel_extension_for_pytorch as ipex
+            bf16_dtype = torch.bfloat16 if opt.bf16 else None
+            transformer = transformer.to(memory_format=torch.channels_last)
+            transformer = ipex.optimize(transformer, level="O1", inplace=True)
+
+            unet = unet.to(memory_format=torch.channels_last)
+            unet = ipex.optimize(unet, level="O1", auto_kernel_selection=True, inplace=True, dtype=bf16_dtype)
+
+            decoder = decoder.to(memory_format=torch.channels_last)
+            decoder = ipex.optimize(decoder, level="O1", auto_kernel_selection=True, inplace=True, dtype=bf16_dtype)
+
+        if opt.torchscript:
+            with torch.no_grad(), additional_context:
+                # get UNET scripted
+                if unet.use_checkpoint:
+                    raise ValueError("Gradient checkpoint won't work with tracing. " +
+                    "Use configs/stable-diffusion/intel/ configs for your model or disable checkpoint in your config.")
+
+                img_in = torch.ones(2, 4, 96, 96, dtype=torch.float32)
+                t_in = torch.ones(2, dtype=torch.int64)
+                context = torch.ones(2, 77, 1024, dtype=torch.float32)
+                scripted_unet = torch.jit.trace(unet, (img_in, t_in, context))
+                scripted_unet = torch.jit.optimize_for_inference(scripted_unet)
+                print(type(scripted_unet))
+                model.model.scripted_diffusion_model = scripted_unet
+
+                # get Decoder for first stage model scripted
+                samples_ddim = torch.ones(1, 4, 96, 96, dtype=torch.float32)
+                scripted_decoder = torch.jit.trace(decoder, (samples_ddim))
+                scripted_decoder = torch.jit.optimize_for_inference(scripted_decoder)
+                print(type(scripted_decoder))
+                model.first_stage_model.decoder = scripted_decoder
+
+        prompts = data[0]
+        print("Running a forward pass to initialize optimizations")
+        uc = None
+        if opt.scale != 1.0:
+            print("Scale at 1.0")
+            uc = model.get_learned_conditioning(batch_size * [negative_prompt])
+        else:
+            print("All fine")
+        if isinstance(prompts, tuple):
+            prompts = list(prompts)
+
+        with torch.no_grad(), additional_context:
+            for _ in range(3):
+                c = model.get_learned_conditioning(prompts)
+            samples_ddim, _ = sampler.sample(S=5,
+                                             conditioning=c,
+                                             batch_size=batch_size,
+                                             shape=shape,
+                                             verbose=False,
+                                             unconditional_guidance_scale=opt.scale,
+                                             unconditional_conditioning=uc,
+                                             eta=opt.ddim_eta,
+                                             x_T=start_code)
+            print("Running a forward pass for decoder")
+            for _ in range(3):
+                x_samples_ddim = model.decode_first_stage(samples_ddim)
+
+    precision_scope = autocast if opt.precision=="autocast" or opt.bf16 else nullcontext
+    with torch.no_grad(), \
+        precision_scope(opt.device), \
+        model.ema_scope():
+            all_samples = list()
+            for n in trange(opt.n_iter, desc="Sampling"):
+                for prompts in tqdm(data, desc="data"):
+                    uc = None
+                    if opt.scale != 1.0:
+                        uc = model.get_learned_conditioning(batch_size * [negative_prompt])
+                    if isinstance(prompts, tuple):
+                        prompts = list(prompts)
+                    c = model.get_learned_conditioning(prompts)
+                    shape = [opt.C, opt.H // opt.f, opt.W // opt.f]
+                    samples, _ = sampler.sample(S=opt.steps,
+                                                     conditioning=c,
+                                                     batch_size=opt.n_samples,
+                                                     shape=shape,
+                                                     verbose=False,
+                                                     unconditional_guidance_scale=opt.scale,
+                                                     unconditional_conditioning=uc,
+                                                     eta=opt.ddim_eta,
+                                                     x_T=start_code,
+                                                     target_index=opt.target_index,
+                                                     original_batch_size=opt.original_batch_size)
+
+                    x_samples = model.decode_first_stage(samples)
+                    x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)
+
+                    for x_sample in x_samples:
+                        x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')
+                        img = Image.fromarray(x_sample.astype(np.uint8))
+                        img = put_watermark(img, wm_encoder)
+                        img_buffer = io.BytesIO()
+                        img.save(img_buffer, format='PNG')
+                        img_buffer.seek(0)
+                        img_data = img_buffer.getvalue()
+                        all_samples.append(img_data)
+                        #base_count += 1
+                        sample_count += 1
+
+    return all_samples
+
 
 if __name__ == "__main__":
     opt = parse_args()
diff --git a/scripts/websocket.py b/scripts/websocket.py
new file mode 100644
index 0000000..de5a532
--- /dev/null
+++ b/scripts/websocket.py
@@ -0,0 +1,77 @@
+from flask import Flask
+from flask_socketio import SocketIO, send
+from txt2img import txt2img
+from argparse import Namespace
+from argparse import ArgumentParser, Namespace
+
+def run_txt2img(ckpt, outdir, H, W, steps, n_iter, device, n_samples, seed, prompt, original_batch_size, target_index, negative_prompt):
+    args = Namespace(
+        ckpt=ckpt,
+        outdir=outdir,
+        H=int(H),
+        W=int(W),
+        steps=int(steps),
+        n_iter=int(n_iter),
+        device=device,
+        n_samples=int(n_samples),
+        seed=int(seed),
+        prompt=prompt,
+        original_batch_size=int(original_batch_size),
+        target_index=int(target_index),
+        negative_prompt=negative_prompt,
+        # Set other parameters to their default values or your desired values
+        plms=False,
+        dpm=False,
+        fixed_code=False,
+        ddim_eta=0.0,
+        C=4,
+        f=8,
+        n_rows=0,
+        scale=7.0,
+        from_file=None,
+        config="configs/stable-diffusion/v2-inference.yaml",
+        precision="autocast",
+        repeat=1,
+        torchscript=False,
+        ipex=False,
+        bf16=False
+    )
+
+    return txt2img(args)
+
+app = Flask(__name__)
+#app.config["SECRET_KEY"] = "nCt9dIHxmiippDbcm5WlsV9Lk3MB0k"
+socketio = SocketIO(app, cors_allowed_origins="*")
+
+# Wrap the main function to accept the seed parameter and return the generated image
+def generate_images(seed,prompt,steps,n_samples):
+    imgs = run_txt2img(
+        ckpt="/models/sd-2-1/model.ckpt",
+        outdir="/cache/",
+        H=768,
+        W=768,
+        steps=steps,
+        n_iter=1,
+        device="cuda",
+        n_samples=n_samples,
+        seed=seed,
+        prompt=prompt,
+        original_batch_size=-1,
+        target_index=-1,
+        negative_prompt="ugly, duplicate, morbid, mutilated, out of frame, extra fingers, mutated hands, poorly drawn hands, poorly drawn face, mutation, deformed, ugly, blurry, bad anatomy, bad proportions, extra limbs, cloned face, disfigured, out of frame, ugly, extra limbs, bad anatomy, gross proportions, malformed limbs, missing arms, missing legs, extra arms, extra legs, mutated hands, fused fingers, too many fingers, long neck"
+    )
+    return imgs
+
+# The WebSocket event for streaming the generated image
+@socketio.on('request_image')
+def stream_images(seed,prompt,steps,n_samples):
+    imgs = generate_images(seed,prompt,steps,n_samples)
+    for img in imgs:
+        socketio.emit('image', img)
+
+if __name__ == "__main__":
+    parser = ArgumentParser()
+    parser.add_argument('-p', '--port', default=5000, type=int,
+                        help='Port to run the server on (default: 5000)')
+    args = parser.parse_args()
+    socketio.run(app, host='127.0.0.1', port=args.port, allow_unsafe_werkzeug=True)
\ No newline at end of file
